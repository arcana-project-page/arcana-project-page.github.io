<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Arcana: Improving Multi-modal Large Language Model through Boosting Vision Capabilities</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Centering Image Example</title>
  <style>
      .item {
          text-align: center; /* Center the contents inside the item div */
      }
      .item img {
          display: inline-block; /* Make the image inline-block to apply text-align */
          max-width: 65%; /* Ensure the image scales with the container */
          height: auto; /* Maintain the aspect ratio */
      }
      .subtitle.has-text-centered {
          margin-top: 1rem; /* Add some margin between the image and the subtitle */
      }
  </style>
</head>


<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Arcana: Improving Multi-modal Large Language Model through Boosting Vision Capabilities</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=a3FI8c4AAAAJ" target="_blank">Yanpeng Sun</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com.hk/citations?user=oyfu0pgAAAAJ&hl=zh-CN" target="_blank">Huaxin Zhang</a><sup>2,3*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=CUGCp3sAAAAJ" target="_blank">Qiang Chen</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=PSzJxD8AAAAJ" target="_blank">Xinyu Zhang</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com.hk/citations?user=ky_ZowEAAAAJ&hl=zh-CN" target="_blank">Nong Sang</a><sup>3</sup>,</span>
                        <span class="author-block">
                          <a href="https://scholar.google.com.hk/citations?user=z5SPCmgAAAAJ&hl=zh-CN" target="_blank">Jingdong Wang</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=L6J2V3sAAAAJ" target="_blank">Zechao Li</a><sup>1&#9993;</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Nanjing University of Science and Technology,&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>2</sup>Baidu VIS,</span> <br>
                    <span class="author-block"><sup>3</sup>Huazhong University of Science and Technology</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution&nbsp;&nbsp;&nbsp;&#9993;Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/syp115/Arcana" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span>HuggingFace</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/syp115/Arcana_VG-COCO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/syp2ysy/Arcana" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            The <b>visual perception capabilities </b> of MLLMs directly impact their performance. It is well-known that the main factors influencing MLLMs' visual perception are <b>data</b> and <b>structure</b>.
              Arcana aims to enhance the visual perception capabilities of MLLMs by addressing both of these aspects.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Motivation carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/motivation.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Sampled some VQA examples involving color, quantity, small objects, and localization
          tasks, showcasing the importance of visual recognition capabilities for multimodal language models
          (MLLMs).
        </h2>
      </div>
      <div class="item">
        <img src="static/images/motivation.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Sampled some VQA examples involving color, quantity, small objects, and localization
          tasks, showcasing the importance of visual recognition capabilities for multimodal language models
          (MLLMs).
        </h2>
        
      </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End Motivation carousel -->



<!-- Dataset Contribution -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">High-quality and Fine-grained Instruction Dataset</h2>
        <div class="content has-text-justified">
          <p>
            On the data side, there is a scarcity of open-source data, and the available multimodal datasets contain limited visual components, preventing MLLMs from gaining sufficient visual perception capabilities from these sources.  To this end, we have developed a data engine to annotate multimodal data that ensures a diversity of visual factors. The specific process is as follows:
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Dataset Contribution -->


<!-- Image Dataset -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/dataset_example.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          LLaVA’s coarse-grained caption v.s. Our fine-grained caption. Important visual recognitionrelated description are highlighted in orange.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/dataset_engine.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The Arcana data engine involves three crucial steps:
          (1) initializing and filtering image
          annotations,
          (2) obtaining diverse attribute annotations for each annotated region, and
          (3) using a
          large language model to integrate these visual annotations into different types of captions.
        </h2>
      </div>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image dataset -->

<!-- Model -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model</h2>
        <div class="content has-text-justified">
          <p>
            On the structural side, the language-driven decoder couples visual and language modalities within the same space, disregarding their unique characteristics and potentially causing information confusion or blurring.
             Furthermore, the frozen visual encoder cannot provide robust visual features, and directly fine-tuning it with a small dataset can affect its generalization capabilities.
              Toward this end, Arcana introduces MM-LoRA, which constructs a multimodal decoder to preserve the unique characteristics of different modalities.
               We also propose a Query Ladder adapter (QLadder) for the visual encoder, which retains the pre-trained image encoder's capabilities while introducing a small number of visual tokens to significantly enhance the model's ability to learn and represent visual information.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Model -->

<!-- Model carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/framework.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          (a) The architecture of the Arcana. (b) The training pipeline of Arcana. MM-LoRA is optional during the pre-training phase.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/mmlora.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          (a) The farmework of MM-LoRA vs. LoRA. MM-LoRA introduces two new hyperparameters, &beta; and &gamma;
          , to control the ranks of the visual and language LoRAs, respectively. Notably,
          we set &beta; + &gamma; = 1 to ensure that MM-LoRA has the same number of parameters as LoRA. (b) The
          architecture of the visual encoder includes the QLadder adapter and CLIP. The QLadder adapter
          consists of cross-attention and FFN layers, with weights initialized from those of CLIP.
       </h2>
     </div>

    </div>
  </div>
</div>
</div>
</section>
<!-- End Model carousel -->


<!-- Model -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            Our Arcana model achieves competitive performance among the exisiting 7B models.
            To validate the effectiveness of QLadder and MM-LoRA, we designed a series of experiments.
            Additionally, to ensure fairness, we use only LLAVA-v1.5 data for the ablation experiments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Model -->

<!-- Experiments carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_vqa.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Performance on General Visual Question Answering benchmarks.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_mm.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Performance on Large Vision-Language Models (LVLM) benchmarks.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_ablation.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Ablation study.
       </h2>
     </div>
     <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_attn.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
      </h2>
      </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/exp_quality.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
    </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- Experiments carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
