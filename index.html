<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Arcana">
  <meta property="og:title" content="Arcana: Improving Multi-modal Large Language Model through Boosting Vision Capabilities"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Arcana: Improving Multi-modal Large Language Model through Boosting Vision Capabilities</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Centering Image Example</title>
  <style>
      .item {
          text-align: center; /* Center the contents inside the item div */
      }
      .item img {
          display: inline-block; /* Make the image inline-block to apply text-align */
          max-width: 70%; /* Ensure the image scales with the container */
          height: auto; /* Maintain the aspect ratio */
      }
      .subtitle.has-text-centered {
          margin-top: 1rem; /* Add some margin between the image and the subtitle */
      }
  </style>
</head>


<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Arcana: Improving Multi-modal Large Language Model through Boosting Vision Capabilities</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=a3FI8c4AAAAJ" target="_blank">Yanpeng Sun</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com.hk/citations?user=oyfu0pgAAAAJ&hl=zh-CN" target="_blank">Huaxin Zhang</a><sup>2,3*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=CUGCp3sAAAAJ" target="_blank">Qiang Chen</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=PSzJxD8AAAAJ" target="_blank">Xinyu Zhang</a><sup>2</sup>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com.hk/citations?user=ky_ZowEAAAAJ&hl=zh-CN" target="_blank">Nong Sang</a><sup>3</sup>,</span>
                        <span class="author-block">
                          <a  target="_blank">Gang Zhang</a><sup>2</sup>,</span>
                        <span class="author-block">
                          <a href="https://scholar.google.com.hk/citations?user=z5SPCmgAAAAJ&hl=zh-CN" target="_blank">Jingdong Wang</a><sup>2&#9993;</sup>,</span>
                          <span class="author-block">
                            <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=L6J2V3sAAAAJ" target="_blank">Zechao Li</a><sup>1&#9993;</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Nanjing University of Science and Technology,&nbsp;&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>2</sup>Baidu VIS,</span> <br>
                    <span class="author-block"><sup>3</sup>Huazhong University of Science and Technology</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution&nbsp;&nbsp;&nbsp;&#9993;Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                    <!-- ArXiv abstract Link Model -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.13733" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper(Arcana)</span>
                      </a>
                    </span>
                    <!-- ArXiv abstract Link Data-->
                    <span class="link-block">
                      <a href="https://arxiv.org/pdf/2409.13540" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>Paper(DataEngine)</span>
                    </a>
                  </span>
                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/syp2ysy/Arcana" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/syp115/Arcana_VG-COCO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                    <!-- Hugging Face link
                    <span class="link-block">
                      <a href="https://huggingface.co/syp115/Arcana" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span>HuggingFace</span>
                    </a>
                  </span> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            The <strong><em>visual perception capabilities</em></strong> of MLLMs directly impact their performance. It is well-known that the main factors influencing MLLMs' visual perception are <strong><em>data</em></strong> and <strong><em>structure</em></strong>.
              Arcana aims to enhance the visual perception capabilities of MLLMs by addressing both of these aspects.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Teaser Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/motivation.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Sampled some VQA examples involving color, quantity, small objects, and localization
          tasks, showcasing the importance of <b>visual recognition capabilities</b> for multimodal language models
          (MLLMs).
        </h2>
    </div>
  </div>
</section>
<!-- End teaser Image -->




<!-- Dataset Contribution -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">High-quality and Fine-grained Instruction Dataset</h2>
        <div class="content has-text-justified">
          <p>
            On the <strong><em>data</em></strong> side, there is a scarcity of open-source data, and the available multimodal datasets contain limited visual components, preventing MLLMs from gaining sufficient visual perception capabilities from these sources.  To this end, we have developed a data engine to annotate multimodal data that ensures a diversity of visual factors. The specific process is as follows:
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Dataset Contribution -->

<!-- Data Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/data_engine_fullanno.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        The Arcana data engine involves three crucial steps:
        (1) augmenting and filtering image basic annotations,
        (2) obtaining text information and description for each annotated region, and
        (3) using a large language model to integrate these visual annotations into different types of captions.
      </h2>
      <!-- <img src="static/images/caption_comparisoin_fullanno.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        LLaVA’s coarse-grained caption v.s. Our fine-grained caption. Important visual recognition related descriptions are highlighted in orange.
      </h2> -->
    </div>
  </div>
</section>
<!-- End Data Image -->

<!-- Experiments carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/caption_comparisoin_fullanno.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          LLaVA’s coarse-grained caption v.s. Our fine-grained caption.<br>
          Important visual recognition related descriptions are highlighted in <span style="color: #39b7a8;">dark green</span>.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/llava_racap_comparison_crop_fullanno.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Comparisons of dense caption between LLaVA-RaCap-COCO118k and ours.<br>
          The hallucination parts are highlighted in <span style="color: red;">red</span>, whereas detailed
          and accurate parts are emphasized in <span style="color: #39b7a8;">dark green</span>.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- Experiments carousel -->

<!--Examples-->
<section class="section" id="Arcana_VG-COCO">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <style>
            table.GeneratedTable {
                width: 100%;
                background-color: #ffffff;
                border-collapse: collapse;
                border-width: 2px;
                border-color: #e0e0e0;
                border-style: solid;
                color: #000000;
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            }
            
            table.GeneratedTable td, table.GeneratedTable th {
                border-width: 1px;
                border-color: #d0d0d0;
                border-style: solid;
                padding: 10px;
                text-align: left;
            }
            
            table.GeneratedTable thead {
                background-color: #f0f0f0;
                color: #000000;
            }
        
            .schedule-table {
                width: 60%; 
                margin: 0 auto; 
            }
        
            .schedule-table td {
                padding: 5px 10px;
                text-align: left;
            }
        
            .schedule-table .completed {
                color: green;
                font-weight: bold;
            }
        
            .schedule-table .pending {
                color: orange;
                font-weight: bold;
            }
        
            .schedule-table .task-icon {
                width: 20px;
                vertical-align: middle;
                margin-right: 5px;
            }
        </style>        
          <div class="column is-six-fifths" width="80%">
            <h3 class="title is-4 has-text-centered">
              <a href="https://huggingface.co/datasets/syp115/Arcana_VG_COCO" target="_blank" style="text-decoration: underline; color: inherit;">Arcana_VG-COCO</a>
            </h3>
            <div class="table-container">
              <table class="GeneratedTable">
                <thead>
                  <tr>
                    <th>File</th>
                    <th>Type</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><code>captions.json</code></td>
                    <td>Image-level captions</td>
                    <td>
                      The <code>captions.json</code> file contains <code>86k</code> conversations about image-level detail/short captions generated by Arcana's data engine. Each element in the JSONL file is formatted as follows:
                      <pre>
    {
      "id": "image_id",
      "image": "image_path",,
      "conversations":[
        { "from": "human", "value": "&lt;image&gt;\nProvide a detailed description
        of the following image" },
        { "from": "gpt", "value": "The scene depicts an airport setting with a 
        sizable white and green airplane, likely a jumbo jet, stationed on the
        runway. The airplane showcases a color scheme of red and green,
        highlighting its unique look.."}
      ]
    }
                      </pre>
                    </td>
                  </tr>
                  <tr>
                    <td><code>region_captions.json</code></td>
                    <td>Region-level captions</td>
                    <td>
                      The <code>region_captions.json</code> file contains <code>86k</code> conversations about region-level captions generated by Arcana's data engine. Each element in the JSONL file is formatted as follows:
                      <pre>
    {
      "id": "image_id",
      "image": "image_path",,
      "conversations":[
        { "from": "human", "value": "&lt;image&gt;\nProvide a comprehensive
        depiction of the area bounded by [0.286, 0.342, 0.658, 0.484]." },
        {"from": "gpt", "value": "The area features a large red double-decker
        bus with a vibrant advertisement displayed on its side.."},
         ...
      ]
    }
                      </pre>
                    </td>
                  </tr>
                  <tr>
                    <td><code>detections.json</code></td>
                    <td>Detections</td>
                    <td>
                      The <code>detections.json</code> file contains <code>86k</code> conversations about object detections generated by Arcana's data engine. Each element in the JSONL file is formatted as follows:
                      <pre>
    {
      "id": "image_id",
      "image": "image_path",,
      "conversations":[
        { "from": "human", "value": "&lt;image&gt;\nDetect the category encompasses
        the region defined by the coordinates [0.324, 0.523, 0.338, 0.538]?" },
        {"from": "gpt", "value": "According to the taxonomy, this region is 
        categorized as a window."},
        ...
      ]
    }
                      </pre>
                    </td>
                  </tr>
                  <tr>
                    <td><code>ocrs.json</code></td>
                    <td>OCRs</td>
                    <td>
                      The <code>ocrs.json</code> file contains <code>13k</code> conversations about OCR content in image generated by Arcana's data engine. Each element in the JSONL file is formatted as follows:
                      <pre>
    {
      "id": "image_id",
      "image": "image_path",,
      "conversations":[
        { "from": "human", "value": "&lt;image&gt;\nProvide the text in 
        the area bounded by [0.142, 0.863, 0.225, 0.892]." },
        { "from": "gpt", "value": "The text is 2726B2V."},
        { "from": "human", "value": "Tell me where the text 2726B2V is in
         this picture." },
        { "from": "gpt", "value": "The text is in [0.142, 0.863, 0.225, 0.892]." }
      ]
    }
                      </pre>
                    </td>
                  </tr>
                </tbody>
              </table>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Model -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Model</h2>
        <div class="content has-text-justified">
          <p>
            On the <strong><em>structural</em></strong> side, the language-driven decoder couples visual and language modalities within the same space, disregarding their unique characteristics and potentially causing information confusion or blurring.
             Furthermore, the frozen visual encoder cannot provide robust visual features, and directly fine-tuning it with a small dataset can affect its generalization capabilities.
              Toward this end, Arcana introduces MM-LoRA, which constructs a multimodal decoder to preserve the unique characteristics of different modalities.
               We also propose a Query Ladder adapter (QLadder) for the visual encoder, which retains the pre-trained image encoder's capabilities while introducing a small number of visual tokens to significantly enhance the model's ability to learn and represent visual information.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Model -->



<!-- Model Image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/framework_new.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          (a) The architecture of the <b>Arcana</b>. <br>(b) The training pipeline of Arcana. MM-LoRA is optional during the pre-training phase.
        </h2>
      <img src="static/images/mmlora_new.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          (a) The farmework of <b>MM-LoRA</b> vs. <b>LoRA</b>. MM-LoRA introduces two new hyperparameters, &beta; and &gamma;
          , to control the ranks of the visual and language LoRAs, respectively. Notably,
          we set &beta; + &gamma; = 1 to ensure that MM-LoRA has the same number of parameters as LoRA. <br>
          (b) The
          architecture of the visual encoder includes the <b>QLadder</b> adapter and CLIP. The QLadder adapter
          consists of cross-attention and FFN layers, with weights initialized from those of CLIP.
       </h2>
    </div>
  </div>
</section>
<!-- End Model Image -->



<!-- Model -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            Our Arcana model achieves competitive performance among the exisiting 7B models.
            To validate the effectiveness of QLadder and MM-LoRA, we designed a series of experiments.
            Additionally, to ensure fairness, we use only LLAVA-v1.5 data for the ablation experiments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Model -->

<!-- Experiments carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_vqa.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Performance on General Visual Question Answering benchmarks.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_mm.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Performance on Large Vision-Language Models (LVLM) benchmarks.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_ablation.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Ablation study.
       </h2>
     </div>
     <div class="item">
        <!-- Your image here -->
        <img src="static/images/exp_attn.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
      </h2>
      </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/exp_quality.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
    </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- Experiments carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
